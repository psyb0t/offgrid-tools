#!/bin/bash

# Get script directory first
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Set up logging first - start fresh each time
LOG_FILE="$SCRIPT_DIR/download.sh.log"
> "$LOG_FILE"  # Clear the log file

# Custom log function that writes to both stdout and file
log() {
    echo "$@"
    echo "$@" >> "$LOG_FILE"
}

# Parse command line arguments
MAX_JOBS=10  # Default value
if [[ $# -gt 0 ]]; then
    if [[ "$1" =~ ^[0-9]+$ ]] && [[ "$1" -gt 0 ]]; then
        MAX_JOBS="$1"
    else
        log "Usage: $0 [max_parallel_jobs]"
        log "  max_parallel_jobs: Number of concurrent downloads (default: 10, must be positive integer)"
        exit 1
    fi
fi

log "=== Downloading All Linux Packages ==="
log "Log file: $LOG_FILE"
log "Started at: $(date)"
log ""
log "This will download and save the following package groups:"
log "  ğŸ³ Docker - container runtime and development tools"
log "  ğŸ“± ADB - android debug bridge and development tools"
log "  ğŸ¹ Go - programming language and tools"
log "  ğŸ Python - programming language and tools"
log "  ğŸ“ Development tools - nano, geany, terminator"
log "  ğŸ–¥ï¸  VirtualBox - virtualization platform"
log "  ğŸŒ Chromium - web browser"
log "  ğŸ¨ GIMP - image editor"
log "  ğŸ¬ FFmpeg - multimedia framework"
log "  âš™ï¸  Supervisor - process control system"
log ""

# Check Docker availability first
if ! command -v docker &> /dev/null; then
    log "âŒ Docker is not installed or not in PATH"
    log "This script requires Docker to create a clean Ubuntu environment for package resolution."
    log "Please install Docker first: https://docs.docker.com/get-docker/"
    exit 1
fi

if ! docker info &> /dev/null; then
    log "âŒ Docker daemon is not running or not accessible"
    log "Please start Docker daemon or check Docker permissions"
    exit 1
fi

log "âœ… Docker is available and running"
log ""

# Detect OS version and architecture
if [[ -f /etc/os-release ]]; then
    source /etc/os-release
    OS_NAME="${ID}"
    OS_VERSION="${VERSION_ID}"
    OS_CODENAME="${VERSION_CODENAME:-${UBUNTU_CODENAME}}"
else
    log "âŒ Could not detect OS version from /etc/os-release"
    exit 1
fi

ARCH=$(dpkg --print-architecture)
OS_DIR="${OS_NAME}-${OS_VERSION}-${ARCH}"
DATA_DIR="${SCRIPT_DIR}/data/${OS_DIR}"

log "ğŸ–¥ï¸  Detected system:"
log "  OS: ${OS_NAME} ${OS_VERSION} (${OS_CODENAME})"
log "  Architecture: ${ARCH}"
log "  Data directory: ${DATA_DIR}"
log "  Max parallel downloads: ${MAX_JOBS}"
log ""

# Create output dir and handle existing files
mkdir -p "$DATA_DIR"

# Check if data directory has .deb files
if [[ -d "$DATA_DIR" ]] && ls "$DATA_DIR"/*.deb >/dev/null 2>&1; then
    existing_count=$(ls "$DATA_DIR"/*.deb 2>/dev/null | wc -l)
    log "ğŸ“¦ Found $existing_count existing .deb files in data directory"
    read -p "Do you want to start fresh (delete existing files)? y/N: " -n 1 -r
    echo ""
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        log "ğŸ§¹ Cleaning up existing data directory..."
        rm -rf "$DATA_DIR"/*.deb 2>/dev/null || true
        log "âœ… Data directory cleaned"
    else
        log "âœ… Keeping existing files - will skip downloading duplicates"
    fi
fi

cd "$DATA_DIR"

# Check if Docker is available
if ! command -v docker >/dev/null 2>&1; then
    log "âŒ Docker not found. Install Docker first."
    exit 1
fi

# Define ALL packages to check individually
PACKAGES=(
    "docker-ce"
    "docker-ce-cli"
    "containerd.io"
    "docker-buildx-plugin"
    "docker-compose-plugin"
    "android-tools-adb"
    "android-tools-fastboot"
    "android-sdk-platform-tools-common"
    "golang-go"
    "python3"
    "python3-pip"
    "python3-venv"
    "nano"
    "chromium-browser"
    "gimp"
    "geany"
    "terminator"
    "supervisor"
    "virtualbox-7.1"
    "ffmpeg"
)

log "ğŸ³ Starting clean Docker container in background..."

# Use Ubuntu base image
docker_image="${OS_NAME}:${OS_VERSION}"
log "  Using Ubuntu base image: $docker_image"

container_name="offgrid-tools-deb-downloader"

# Start container in background
docker run -d --name "$container_name" "$docker_image" sleep 3600 >/dev/null

# Track background processes
bg_pids=()

# Track if cleanup has already run
cleanup_done=false

# Function to cleanup container and kill background jobs on exit
cleanup() {
    # Prevent duplicate cleanup
    if [[ "$cleanup_done" == "true" ]]; then
        return
    fi
    cleanup_done=true

    log ""
    log "ğŸ›‘ Stopping downloads and cleaning up..."

    # Kill tracked background processes
    if [[ ${#bg_pids[@]} -gt 0 ]]; then
        log "Stopping ${#bg_pids[@]} background processes..."
        for pid in "${bg_pids[@]}"; do
            if kill -0 "$pid" 2>/dev/null; then
                kill -TERM "$pid" 2>/dev/null || true
                sleep 1
                kill -KILL "$pid" 2>/dev/null || true
            fi
        done
    fi

    # Kill any remaining processes
    pkill -f "wget.*\.deb" 2>/dev/null || true
    pkill -f "xargs.*download_file" 2>/dev/null || true
    pkill -f "parallel.*download_file" 2>/dev/null || true

    # Clean up Docker container
    if [[ -n "$container_name" ]]; then
        log "ğŸ§¹ Cleaning up Docker container: $container_name"
        if docker ps -q -f name="$container_name" | grep -q .; then
            docker kill "$container_name" 2>/dev/null || true
            sleep 1
        fi
        docker rm -f "$container_name" 2>/dev/null || true
    fi

    # Clean up temp files
    [[ -n "$temp_file" ]] && rm -f "$temp_file" 2>/dev/null || true
    rm -f "/tmp/package_urls_$$" 2>/dev/null || true
    rm -f "$SUCCESS_FILE" "$FAILED_FILE" "$SKIPPED_FILE" 2>/dev/null || true

    log "Session interrupted at: $(date)"
    exit 130
}
# Simple trap - tee was interfering with signals
trap cleanup EXIT INT TERM

log "ğŸ“‹ Setting up repositories in container..."

# Make sure we can still interrupt during container setup

# Setup repositories in the container
log "ğŸ”§ Setting up APT repositories..."

# Run each docker exec command separately so we can see progress
log "ğŸ“¦ Running apt-get update..."
docker exec "$container_name" apt-get update >/dev/null 2>&1

log "ğŸ”§ Installing prerequisites..."
docker exec "$container_name" apt-get install -y ca-certificates curl gnupg software-properties-common wget >/dev/null 2>&1
docker exec "$container_name" mkdir -p /etc/apt/keyrings >/dev/null 2>&1

log "ğŸ³ Setting up Docker repository..."
docker exec "$container_name" curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc >/dev/null 2>&1
docker exec "$container_name" chmod a+r /etc/apt/keyrings/docker.asc >/dev/null 2>&1
docker exec "$container_name" bash -c "echo 'deb [arch=amd64 signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $OS_CODENAME stable' > /etc/apt/sources.list.d/docker.list" >/dev/null 2>&1

log "ğŸ¹ Setting up Go PPA..."
docker exec "$container_name" add-apt-repository -y ppa:longsleep/golang-backports >/dev/null 2>&1

log "ğŸ“¦ Setting up VirtualBox repository..."
docker exec "$container_name" wget -O- https://www.virtualbox.org/download/oracle_vbox_2016.asc 2>/dev/null | docker exec -i "$container_name" gpg --dearmor --yes --output /usr/share/keyrings/oracle-virtualbox-2016.gpg >/dev/null 2>&1
docker exec "$container_name" bash -c "echo 'deb [arch=amd64 signed-by=/usr/share/keyrings/oracle-virtualbox-2016.gpg] https://download.virtualbox.org/virtualbox/debian $OS_CODENAME contrib' > /etc/apt/sources.list.d/virtualbox.list" >/dev/null 2>&1

log "ğŸ”„ Final apt-get update..."
docker exec "$container_name" apt-get update >/dev/null 2>&1
log "âœ… Repository setup complete!"

# Repository setup is complete (commands ran synchronously)

# Package checking loop - handle errors manually

log "ğŸ” Checking packages individually for dependency resolution..."
log "ğŸ“Š Total packages to check: ${#PACKAGES[@]}"

successful_packages=()
failed_packages=()
all_urls=()

# Check each package individually
package_count=0
for package in "${PACKAGES[@]}"; do
    ((package_count++))
    log "  [$package_count/${#PACKAGES[@]}] Checking $package..."

    # Step 1: Test if package exists
    log "    â†’ Checking if $package exists..."
    docker exec "$container_name" apt-cache show "$package" >/dev/null 2>&1
    if [[ $? -ne 0 ]]; then
        log "  [$package_count/${#PACKAGES[@]}] âŒ $package - package not found"
        failed_packages+=("$package")
        continue
    fi
    log "    â†’ Package exists"

    # Step 2: Get raw apt-get output
    log "    â†’ Getting download URLs..."
    temp_raw="/tmp/raw_apt_$$"
    docker exec "$container_name" apt-get --print-uris --yes --reinstall install "$package" > "$temp_raw" 2>&1
    if [[ $? -ne 0 ]]; then
        log "  [$package_count/${#PACKAGES[@]}] âŒ $package - apt-get failed"
        log "    â†’ Error: $(head -3 "$temp_raw" | tail -1)"
        failed_packages+=("$package")
        rm -f "$temp_raw"
        continue
    fi

    # Step 3: Extract URLs from output
    temp_urls="/tmp/urls_$$"
    grep "^'" "$temp_raw" > "$temp_urls" 2>/dev/null
    if [[ $? -ne 0 ]]; then
        log "  [$package_count/${#PACKAGES[@]}] âŒ $package - no download URLs found in output"
        log "    â†’ Debug: First 3 lines of apt-get output:"
        head -3 "$temp_raw" | sed 's/^/    â†’ /'
        failed_packages+=("$package")
        rm -f "$temp_raw" "$temp_urls"
        continue
    fi

    # Step 4: Clean URLs
    temp_clean="/tmp/clean_$$"
    cut -d"'" -f2 "$temp_urls" > "$temp_clean" 2>/dev/null
    if [[ $? -ne 0 ]]; then
        log "  [$package_count/${#PACKAGES[@]}] âŒ $package - failed to extract URLs"
        failed_packages+=("$package")
        rm -f "$temp_raw" "$temp_urls" "$temp_clean"
        continue
    fi
    
    # Show URL count
    if [[ -s "$temp_clean" ]]; then
        url_count=$(wc -l < "$temp_clean")
        log "    â†’ Found $url_count download URLs"
    fi


    # Step 5: Verify we got URLs
    if [[ -s "$temp_clean" ]]; then
        url_count=$(wc -l < "$temp_clean")
        log "  [$package_count/${#PACKAGES[@]}] âœ… $package resolved successfully ($url_count URLs)"
        successful_packages+=("$package")
        # Add URLs to our collection
        while IFS= read -r url; do
            [[ -n "$url" ]] && all_urls+=("$url")
        done < "$temp_clean"
    else
        log "  [$package_count/${#PACKAGES[@]}] âŒ $package - no clean URLs extracted"
        failed_packages+=("$package")
    fi

    # Cleanup
    rm -f "$temp_raw" "$temp_urls" "$temp_clean"

    # Clean up temp file
    rm -f "/tmp/package_urls_$$" 2>/dev/null
done

# Remove duplicate URLs
log ""
log "ğŸ”§ Removing duplicate package URLs..."
declare -A seen_urls
unique_urls=()
for url in "${all_urls[@]}"; do
    if [[ -z "${seen_urls[$url]}" ]]; then
        seen_urls[$url]=1
        unique_urls+=("$url")
    fi
done

log "ğŸ“Š Package Analysis Summary:"
log "  âœ… Successfully resolved: ${#successful_packages[@]} packages"
log "  âŒ Failed to resolve: ${#failed_packages[@]} packages"
log "  ğŸ“¦ Total unique packages to download: ${#unique_urls[@]}"
log ""

if [[ ${#successful_packages[@]} -eq 0 ]]; then
    log "âŒ No packages could be resolved. Check your repositories and network connection."
    exit 1
fi

# Clean up Docker container now that we have all URLs
log "ğŸ§¹ Cleaning up Docker container (no longer needed)..."
docker stop "$container_name" >/dev/null 2>&1 || true
docker rm "$container_name" >/dev/null 2>&1 || true
container_name=""  # Clear so cleanup function doesn't try again

log "âœ… Successfully resolved packages:"
for pkg in "${successful_packages[@]}"; do
    log "  â€¢ $pkg"
done

if [[ ${#failed_packages[@]} -gt 0 ]]; then
    log ""
    log "âŒ Failed packages (will be skipped):"
    for pkg in "${failed_packages[@]}"; do
        log "  â€¢ $pkg"
    done
fi

log ""
log "ğŸš€ Starting parallel download of ${#unique_urls[@]} packages..."

# Temporarily disable exit on error for downloads
set +e

# Create temp files for tracking results
mkdir -p /tmp/offgrid-tools
SUCCESS_FILE="/tmp/offgrid-tools/.apps-linux-deb-download-success"
FAILED_FILE="/tmp/offgrid-tools/.apps-linux-deb-download-failed"
SKIPPED_FILE="/tmp/offgrid-tools/.apps-linux-deb-download-skipped"

# Clear previous results
> "$SUCCESS_FILE"
> "$FAILED_FILE"
> "$SKIPPED_FILE"

# Create a function to download a single file
download_file() {
    local url="$1"
    local index="$2"
    local total="$3"
    local filename=$(basename "$url")

    # Decode URL-encoded filename for local storage (proper URL decoding)
    local decoded_filename=$(python3 -c "import urllib.parse; print(urllib.parse.unquote('$filename'))" 2>/dev/null || echo "$filename")

    # Check if file already exists (try both encoded and decoded names)
    if [[ -f "$filename" ]] || [[ -f "$decoded_filename" ]]; then
        # Use log function for download status
        log "[$index/$total] â­ï¸  $url -> $decoded_filename (already exists)"
        echo "$decoded_filename" >> "$SKIPPED_FILE"
        return 0
    fi

    # Use wget with timeout and retry options, save with decoded filename
    local wget_output
    wget_output=$(timeout 300 wget --quiet --tries=3 --timeout=60 -O "$decoded_filename" "$url" 2>&1)
    local wget_result=$?

    if [[ $wget_result -eq 0 ]]; then
        log "[$index/$total] âœ… $url -> $decoded_filename"
        echo "$decoded_filename" >> "$SUCCESS_FILE"
        return 0
    else
        # Clean up partial download
        rm -f "$decoded_filename" 2>/dev/null

        # Provide better error reporting
        if [[ $wget_result -eq 124 ]]; then
            log "[$index/$total] âŒ $url (timeout)"
        elif echo "$wget_output" | grep -q "404\|not found"; then
            log "[$index/$total] âŒ $url (404 not found)"
        elif echo "$wget_output" | grep -q "Name or service not known"; then
            log "[$index/$total] âŒ $url (DNS error)"
        else
            log "[$index/$total] âŒ $url (exit code: $wget_result)"
        fi
        echo "$decoded_filename" >> "$FAILED_FILE"
        return 1
    fi
}

# Export functions so they're available to parallel processes
export -f download_file
export -f log
export LOG_FILE SUCCESS_FILE FAILED_FILE SKIPPED_FILE

# Download all URLs in parallel with limited concurrent jobs
total_urls=${#unique_urls[@]}

log "Starting parallel downloads (max $MAX_JOBS concurrent)..."

# Create a temporary file to store URLs with indices
temp_file=$(mktemp)
for i in "${!unique_urls[@]}"; do
    echo "${unique_urls[$i]} $((i + 1)) $total_urls" >> "$temp_file"
done

# Use xargs to run downloads in parallel
if command -v parallel >/dev/null 2>&1; then
    # Use GNU parallel if available (better performance)
    log "Using GNU parallel for downloads..."
    log "Press CTRL+C to stop downloads..."
    parallel -j "$MAX_JOBS" --colsep ' ' download_file {1} {2} {3} :::: "$temp_file" &
    parallel_pid=$!
    bg_pids+=("$parallel_pid")
    wait $parallel_pid
else
    # Fallback to xargs with parallel processing
    log "Using xargs for parallel downloads..."
    log "Press CTRL+C to stop downloads..."
    cat "$temp_file" | xargs -n 3 -P "$MAX_JOBS" bash -c 'download_file "$1" "$2" "$3"' _ &
    xargs_pid=$!
    bg_pids+=("$xargs_pid")
    wait $xargs_pid
fi

# Clean up temp file
rm -f "$temp_file"

# Count download results from temp files (ignore empty lines)
download_success=0
download_skipped=0
download_failed=0

if [[ -f "$SUCCESS_FILE" ]]; then
    download_success=$(grep -c . "$SUCCESS_FILE" 2>/dev/null || echo 0)
fi

if [[ -f "$SKIPPED_FILE" ]]; then
    download_skipped=$(grep -c . "$SKIPPED_FILE" 2>/dev/null || echo 0)
fi

if [[ -f "$FAILED_FILE" ]]; then
    download_failed=$(grep -c . "$FAILED_FILE" 2>/dev/null || echo 0)
fi

# Count total files we have now
total_files=$(ls *.deb 2>/dev/null | wc -l || echo 0)

# Clean up temp files
rm -f "$SUCCESS_FILE" "$FAILED_FILE" "$SKIPPED_FILE" 2>/dev/null || true

# Re-enable exit on error
set -e

# Show download summary
log ""
log "ğŸ“Š Download Summary:"
if ls *.deb >/dev/null 2>&1; then
    log "âœ… .deb packages available:"
    ls -lh *.deb
    log ""
    log "ğŸ“¦ Total directory size:"
    du -sh .
else
    log "âŒ No .deb files found!"
fi

log ""
log "ğŸ“ Files are saved in: $(pwd)"
log ""
log "ğŸ“‹ Final Summary:"
log "  âœ… Packages resolved: ${#successful_packages[@]}/${#PACKAGES[@]}"
log "  ğŸ“¦ Total .deb files: $total_files"
log ""
log "ğŸ“¥ Download Results:"
log "  âœ… Downloaded: $download_success"
log "  â­ï¸  Skipped (already existed): $download_skipped"
log "  âŒ Failed: $download_failed"

if [[ ${#failed_packages[@]} -gt 0 ]]; then
    log ""
    log "âš ï¸  The following packages could not be resolved and were skipped:"
    for pkg in "${failed_packages[@]}"; do
        log "    â€¢ $pkg - check if package name is correct or repository is available"
    done
fi

# Only show package info if we actually have successful packages
if [[ ${#successful_packages[@]} -gt 0 ]] && [[ $total_files -gt 0 ]]; then
    log ""
    log "ğŸ“‹ Successfully downloaded packages are ready for offline use:"
    log "   Run ./install.sh to install them"
elif [[ ${#successful_packages[@]} -gt 0 ]]; then
    log ""
    log "âš ï¸  Package dependency resolution succeeded but downloads failed."
    log "   This may be due to outdated repository package versions."
    log "   Run the script again - repositories may have updated."
else
    log ""
    log "âŒ No packages were successfully processed."
    log "   Check your internet connection and repository access."
fi

# Only show completion if not interrupted
if [[ "$cleanup_done" != "true" ]]; then
    log ""
    log "Session completed at: $(date)"
    log "Full log saved to: $LOG_FILE"
fi
